{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a355ed2a-5746-47f3-a542-2f4c7005853a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Alzheimers Prediction Data Pre-Processing. The Ugly Way\n",
    "\n",
    "\n",
    "- Pre-Process raw data to a SQL compliant format. No feature engineering here\n",
    "- Split into train and test tables\n",
    "- Load to tables in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10889b49-ca38-40e7-b965-70c18c49a807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, to_utc_timestamp\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca14b99-c7c2-4aed-8380-a3706baf17b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(\"../project_config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "catalog_name = config[\"catalog_name\"]\n",
    "schema_name = config[\"schema_name\"]\n",
    "target = config[\"target\"]\n",
    "print(catalog_name, schema_name, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ac4333-1188-465a-8551-5042b1b70ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Only works in a Databricks environment if the data is there\n",
    "# to put data there, create volume and run databricks fs cp <path> dbfs:/Volumes/mlops_dev/<schema_name>/<volume_name>/\n",
    "# filepath = f\"/Volumes/{catalog_name}/{schema_name}/data/data.csv\"\n",
    "# Load the data\n",
    "# df = pd.read_csv(filepath)\n",
    "\n",
    "# Works both locally and in a Databricks environment\n",
    "filepath = \"../data/alzheimers_prediction_dataset.csv\"\n",
    "# Load the data\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Works both locally and in a Databricks environment\n",
    "# df = spark.read.csv(f\"/Volumes/{catalog_name}/{schema_name}/data/data.csv\", header=True, inferSchema=True).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3cc5a0a-d84e-424a-a220-2bb22e6ed28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d101e4-b70f-4639-a5b9-fb0c2f9d09d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pre-process the data\n",
    "\n",
    "# Make columns names SQL compliant\n",
    "df.columns = [column.replace(\" \",\"_\").lower().replace(\"â€™\",\"\") for column in df.columns]\n",
    "df.columns = [re.sub(r'[^a-zA-Z0-9]', '_', column) for column in df.columns]\n",
    "\n",
    "# Cast features to the correct type (Types are different locally than in databrics, locally they are 32, in Databricks they are 64)\n",
    "\n",
    "num_features = df.select_dtypes(include=['int32','int64','float64','float32']).columns.tolist() \n",
    "cat_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_features.remove(target)\n",
    "for cat_col in cat_features:\n",
    "    df[cat_col] = df[cat_col].astype(\"category\")\n",
    "\n",
    "# Make target 1 or 0\n",
    "df[target] = df[target].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Add df index as id column\n",
    "df['id'] = df.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3339f1b5-e18a-4a2a-acc2-f472e10a98f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8fb7adc-04a9-480c-a27f-b1d9e8e5f729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1da566-4f4f-4c6c-9b6c-0bf54be7f911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relevant_columns = cat_features + num_features + [target] + ['id']\n",
    "df = df[relevant_columns]\n",
    "train_set, test_set = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671129e6-78a9-4579-a32e-992368815932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_set.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e99691f-e30f-47ff-9710-2c0126c11105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb74a30-580c-4de4-9bef-b17baad10444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load to database with update timestamp\n",
    "\n",
    "# Transform Pandas DF to Spark. If it is not already a Spark DF\n",
    "if not isinstance(train_set, pd.DataFrame):\n",
    "    train_set = pd.DataFrame(train_set)\n",
    "if not isinstance(test_set, pd.DataFrame):\n",
    "    test_set = pd.DataFrame(test_set)\n",
    "\n",
    "train_set = spark.createDataFrame(train_set)\n",
    "train_set_with_timestamp = train_set.withColumn(\"update_timestamp_utc\", to_utc_timestamp(current_timestamp(), \"UTC\"))\n",
    "\n",
    "test_set = spark.createDataFrame(test_set)\n",
    "test_set_with_timestamp =  test_set.withColumn(\"update_timestamp_utc\", to_utc_timestamp(current_timestamp(), \"UTC\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323c2272-b10c-415a-a315-099a2bef603d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74601216-dee2-47b4-80e4-4bbb2abe533d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to database\n",
    "train_set_with_timestamp.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.train_set\")\n",
    "test_set_with_timestamp.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.test_set\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Week2-02-Ugly Data processing",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
